{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Maps Reviews Scraper\n",
        "Jalankan sel-sel di bawah ini secara berurutan.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "Ini akan menginstall playwright, pandas, dan konfigurasi environment asynchronous di Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "!pip install playwright pandas nest_asyncio\n",
        "!playwright install chromium\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup Asyncio\n",
        "Google Colab sudah berjalan di dalam loop asyncio, sehingga kita perlu mengaplikasikan `nest_asyncio`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Core Script\n",
        "Berikut adalah gabungan semua modul dan file konfigurasi (utils, scraper, data processor).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re, time, random, os, logging\n",
        "from datetime import datetime, timedelta\n",
        "from playwright.sync_api import sync_playwright\n",
        "\n",
        "SELECTORS_JSON = \"\"\"\n",
        "{\n",
        "    \"search\": {\n",
        "        \"input\": \"input[name='q']\",\n",
        "        \"recommendation_item\": \"div.Nv2PK\",\n",
        "        \"recommendation_link\": \"a.hfpxzc\"\n",
        "    },\n",
        "    \"place_details\": {\n",
        "        \"name\": \"h1.DUwDvf\",\n",
        "        \"rating\": \"div.F7nice span[aria-hidden='true']\",\n",
        "        \"reviews_count\": \"div.F7nice span[aria-label*='ulasan']\",\n",
        "        \"address\": \"[data-item-id='address']\",\n",
        "        \"website\": \"[data-item-id='authority']\",\n",
        "        \"phone\": \"[data-item-id^='phone']\"\n",
        "    },\n",
        "    \"reviews\": {\n",
        "        \"tab_button\": \"button[role='tab'][aria-label*='Ulasan'], button:has-text('Ulasan'), div[role='tab']:has-text('Ulasan')\",\n",
        "        \"sort_button\": \"button[aria-label='Urutkan ulasan'], button[data-value='Urutkan']\",\n",
        "        \"sort_newest\": \"div[role='menuitem'] >> text=Terbaru, div[role='menuitemradio'] >> text=Terbaru\",\n",
        "        \"container\": \"div.m6QErb.DxyBCb.kA9KIf.dS8AEf[tabindex='-1'], div[role='main'] > div.m6QErb[tabindex='-1']\",\n",
        "        \"item\": \"div.jftiEf\",\n",
        "        \"author\": \"div.d4r55\",\n",
        "        \"rating\": \"span.kvMYJc\",\n",
        "        \"text\": \"span.wiI7pd\",\n",
        "        \"date\": \"span.rsqaWe\",\n",
        "        \"more_button\": \"button.w8nwRe\",\n",
        "        \"more_reviews_button\": \"button[aria-label^='Ulasan lainnya']\",\n",
        "        \"owner_reply\": \"div.wiI7pd\"\n",
        "    },\n",
        "    \"xpath_fallbacks\": {\n",
        "        \"name\": \"xpath=//h1[contains(@class, 'DUwDvf')]\",\n",
        "        \"address\": \"xpath=//button[@data-item-id='address']\",\n",
        "        \"website\": \"xpath=//a[@data-item-id='authority']\",\n",
        "        \"phone\": \"xpath=//button[starts-with(@data-item-id, 'phone')]\",\n",
        "        \"rating\": \"xpath=//div[contains(@class, 'F7nice')]//span[@aria-hidden='true']\",\n",
        "        \"reviews_count\": \"xpath=//div[contains(@class, 'F7nice')]//span[contains(@aria-label, 'ulasan')]\",\n",
        "        \"sort_button\": \"xpath=//button[@aria-label='Urutkan ulasan']\",\n",
        "        \"sort_newest\": \"xpath=(//div[@role='menuitem'])[2]\"\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "SELECTORS = json.loads(SELECTORS_JSON)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def random_delay(min_val=None, max_val=None):\n",
        "    \"\"\"Wait for a random duration.\"\"\"\n",
        "    min_d = float(os.getenv(\"MIN_DELAY\", 2)) if min_val is None else min_val\n",
        "    max_d = float(os.getenv(\"MAX_DELAY\", 5)) if max_val is None else max_val\n",
        "    time.sleep(random.uniform(min_d, max_d))\n",
        "\n",
        "def parse_relative_date(date_str):\n",
        "    \"\"\"Convert relative date strings (e.g., '15 jam lalu', '3 minggu lalu') to YYYY-MM-DD.\"\"\"\n",
        "    if not date_str:\n",
        "        return None\n",
        "        \n",
        "    now = datetime.now()\n",
        "    date_str = date_str.lower()\n",
        "    \n",
        "    # Handle 'Baru'\n",
        "    if 'baru' in date_str:\n",
        "        return now.strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "    number = 0\n",
        "    match = re.search(r'(\\d+)', date_str)\n",
        "    if match:\n",
        "        number = int(match.group(1))\n",
        "        \n",
        "    if 'jam' in date_str:\n",
        "        delta = timedelta(hours=number)\n",
        "    elif 'hari' in date_str:\n",
        "        delta = timedelta(days=number)\n",
        "    elif 'minggu' in date_str:\n",
        "        delta = timedelta(weeks=number)\n",
        "    elif 'bulan' in date_str:\n",
        "        delta = timedelta(days=number * 30)\n",
        "    elif 'tahun' in date_str:\n",
        "        delta = timedelta(days=number * 365)\n",
        "    else:\n",
        "        # Fallback to today if unknown\n",
        "        return now.strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "    target_date = now - delta\n",
        "    return target_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def extract_place_id_from_url(url):\n",
        "    \"\"\"Extract Place ID from Google Maps URL using regex patterns.\"\"\"\n",
        "    # Pattern: /data=!4m...!1s(PLACE_ID)!\n",
        "    # Example: ChIJ3-Wr1gL1aS4R6ILT4LEMITg or hex format 0x...:0x...\n",
        "    match = re.search(r'!1s([a-zA-Z0-9_:-]+)(?:!|$)', url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    \n",
        "    # Fallback search for ChIJ\n",
        "    match = re.search(r'(ChIJ[a-zA-Z0-9_-]{10,})', url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "        \n",
        "    return None\n",
        "\n",
        "def extract_lat_long_from_url(url):\n",
        "    \"\"\"Extract Latitude and Longitude from Google Maps URL.\"\"\"\n",
        "    # Pattern: /@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)\n",
        "    # Example: /@-6.2381042,106.7661399,17z/\n",
        "    match = re.search(r'@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)', url)\n",
        "    if match:\n",
        "        return float(match.group(1)), float(match.group(2))\n",
        "    return None, None\n",
        "\n",
        "def load_selectors(filepath=None):\n",
        "    return SELECTORS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup_logger(log_folder=\"logs\"):\n",
        "    \"\"\"Sets up logging to console and file.\"\"\"\n",
        "    if not os.path.exists(log_folder):\n",
        "        os.makedirs(log_folder)\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = os.path.join(log_folder, f\"scrape_log_{timestamp}.txt\")\n",
        "    \n",
        "    logger = logging.getLogger(\"gmaps_scraper\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "    \n",
        "    # Avoid duplicate handlers\n",
        "    if not logger.handlers:\n",
        "        # Formatter\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "        \n",
        "        # File handler\n",
        "        file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
        "        file_handler.setFormatter(formatter)\n",
        "        \n",
        "        # Console handler\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setFormatter(formatter)\n",
        "        \n",
        "        logger.addHandler(file_handler)\n",
        "        logger.addHandler(console_handler)\n",
        "    \n",
        "    return logger\n",
        "\n",
        "import random\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "import os\n",
        "\n",
        "\n",
        "logger = setup_logger()\n",
        "\n",
        "class BrowserManager:\n",
        "    def __init__(self, headless=True):\n",
        "        self.headless = headless\n",
        "        self.ua = UserAgent()\n",
        "        self.browser = None\n",
        "        self.context = None\n",
        "        self.page = None\n",
        "\n",
        "    def start_browser(self):\n",
        "        \"\"\"Starts a fresh browser instance with anti-detection args.\"\"\"\n",
        "        try:\n",
        "            pw = sync_playwright().start()\n",
        "            self.browser = pw.chromium.launch(\n",
        "                headless=self.headless,\n",
        "                args=[\n",
        "                    \"--disable-blink-features=AutomationControlled\",\n",
        "                    \"--no-sandbox\",\n",
        "                    \"--disable-setuid-sandbox\",\n",
        "                    \"--disable-infobars\",\n",
        "                    \"--window-position=0,0\",\n",
        "                    \"--ignore-certifcate-errors\",\n",
        "                    \"--ignore-certifcate-errors-spki-list\",\n",
        "                    \"--user-agent=\" + self.ua.random\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            # Context randomization\n",
        "            viewport_width = random.randint(1280, 1920)\n",
        "            viewport_height = random.randint(720, 1080)\n",
        "            \n",
        "            self.context = self.browser.new_context(\n",
        "                viewport={'width': viewport_width, 'height': viewport_height},\n",
        "                user_agent=self.ua.random\n",
        "            )\n",
        "            \n",
        "            # Initial stealth scripts could be added here if needed\n",
        "            self.page = self.context.new_page()\n",
        "            \n",
        "            # Hide automation traces\n",
        "            self.page.add_init_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "            \n",
        "            logger.info(f\"Browser started successfully. Viewport: {viewport_width}x{viewport_height}\")\n",
        "            return self.page\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to start browser: {e}\")\n",
        "            raise\n",
        "\n",
        "    def close_browser(self):\n",
        "        \"\"\"Closes the browser instance.\"\"\"\n",
        "        if self.browser:\n",
        "            self.browser.close()\n",
        "            logger.info(\"Browser closed.\")\n",
        "\n",
        "    def get_new_context(self):\n",
        "        \"\"\"Creates a fresh context to clear session data.\"\"\"\n",
        "        if self.context:\n",
        "            self.context.close()\n",
        "        \n",
        "        viewport_width = random.randint(1280, 1920)\n",
        "        viewport_height = random.randint(720, 1080)\n",
        "        \n",
        "        self.context = self.browser.new_context(\n",
        "            viewport={'width': viewport_width, 'height': viewport_height},\n",
        "            user_agent=self.ua.random\n",
        "        )\n",
        "        self.page = self.context.new_page()\n",
        "        self.page.add_init_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        logger.info(f\"Fresh context created. Viewport: {viewport_width}x{viewport_height}\")\n",
        "        return self.page\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logger = setup_logger()\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self, output_folder=\"output_data\"):\n",
        "        self.output_folder = output_folder\n",
        "        if not os.path.exists(output_folder):\n",
        "            os.makedirs(output_folder)\n",
        "        self.session_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def process_reviews(self, raw_reviews, place_details=None):\n",
        "        \"\"\"Cleans and formats review data.\"\"\"\n",
        "        if place_details is None:\n",
        "            place_details = {}\n",
        "            \n",
        "        processed = []\n",
        "        for review in raw_reviews:\n",
        "            # Parse date\n",
        "            parsed_date = parse_relative_date(review.get('tanggal_raw'))\n",
        "            \n",
        "            # Filter by date (April 2025 onwards)\n",
        "            if parsed_date and parsed_date < \"2025-04-01\":\n",
        "                continue\n",
        "                \n",
        "            row = {\n",
        "                'place_id': review.get('place_id'),\n",
        "                'place_url': review.get('place_url'),\n",
        "                'nama_tempat': review.get('nama_tempat'),\n",
        "                'latitude': place_details.get('latitude'),\n",
        "                'longitude': place_details.get('longitude'),\n",
        "                'description': place_details.get('description'),\n",
        "                'is_spending': place_details.get('is_spending'),\n",
        "                'reviews': place_details.get('ulasan_total'),\n",
        "                'competitors': place_details.get('competitors'),\n",
        "                'website': place_details.get('website'),\n",
        "                'can_claim': place_details.get('can_claim'),\n",
        "                'owner': place_details.get('owner'),\n",
        "                'featured_image': place_details.get('featured_image'),\n",
        "                'main_category': place_details.get('main_category'),\n",
        "                'categories': place_details.get('categories'),\n",
        "                'total_rating': place_details.get('rating_total'),\n",
        "                'total_reviews': place_details.get('ulasan_total'),\n",
        "                'review_rating': review.get('rating_ulasan'),\n",
        "                'workday_timing': place_details.get('workday_timing'),\n",
        "                'is_temporarily_closed': place_details.get('is_temporarily_closed'),\n",
        "                'is_permanently_closed': place_details.get('is_permanently_closed'),\n",
        "                'closed_on': place_details.get('closed_on'),\n",
        "                'phone': place_details.get('telepon'),\n",
        "                'address': place_details.get('alamat'),\n",
        "                'review_keywords': place_details.get('review_keywords'),\n",
        "                'author_name': review.get('author_name', '').strip(),\n",
        "                'tanggal_review': parsed_date,\n",
        "                'isi_review': review.get('isi_review', '').strip(),\n",
        "                'balasan_pemilik': review.get('balasan_pemilik', '').strip(),\n",
        "                'tanggal_balasan': parse_relative_date(review.get('tanggal_balasan_raw')),\n",
        "                'review_id': review.get('review_id'),\n",
        "                'ingestion_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "            \n",
        "            processed.append(row)\n",
        "            \n",
        "        return processed\n",
        "\n",
        "    def export_to_csv(self, data, name_prefix=\"gmaps_scrape\", mode=\"a\"):\n",
        "        \"\"\"Exports data to a CSV file.\"\"\"\n",
        "        if not data:\n",
        "            logger.warning(\"No data to export.\")\n",
        "            return None\n",
        "            \n",
        "        filename = f\"{name_prefix}_{self.session_timestamp}.csv\"\n",
        "        filepath = os.path.join(self.output_folder, filename)\n",
        "        \n",
        "        df = pd.DataFrame(data)\n",
        "        # Ensure correct column order\n",
        "        cols = [\n",
        "            'place_id', 'place_url', 'nama_tempat', 'latitude', 'longitude', 'address', 'description', 'is_spending',\n",
        "            'reviews', 'total_reviews', 'competitors', 'website', 'can_claim', 'owner', 'featured_image',\n",
        "            'main_category', 'categories', 'total_rating', 'review_rating', 'workday_timing', 'is_temporarily_closed',\n",
        "            'is_permanently_closed', 'closed_on', 'phone', 'review_id', 'review_keywords',\n",
        "            'author_name', 'tanggal_review', 'isi_review', 'balasan_pemilik', 'tanggal_balasan', 'ingestion_time'\n",
        "        ]\n",
        "        \n",
        "        # Keep only columns that actually exist to prevent KeyError\n",
        "        existing_cols = [c for c in cols if c in df.columns]\n",
        "        df = df[existing_cols]\n",
        "        \n",
        "        write_header = not os.path.exists(filepath) or mode == 'w'\n",
        "        df.to_csv(filepath, index=False, mode=mode, header=write_header, encoding='utf-8-sig')\n",
        "        logger.info(f\"Data exported to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    def export_errors(self, errors):\n",
        "        \"\"\"Exports errors to a separate CSV.\"\"\"\n",
        "        if not errors:\n",
        "            return None\n",
        "            \n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"error_report_{timestamp}.csv\"\n",
        "        filepath = os.path.join(self.output_folder, filename)\n",
        "        \n",
        "        df = pd.DataFrame(errors)\n",
        "        df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
        "        logger.info(f\"Error report exported to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        ", extract_lat_long_from_url\n",
        "\n",
        "\n",
        "logger = setup_logger()\n",
        "\n",
        "class GoogleMapsScraper:\n",
        "    def __init__(self, page):\n",
        "        self.page = page\n",
        "        self.selectors = load_selectors()\n",
        "\n",
        "    def search_place(self, name):\n",
        "        \"\"\"Searches for a place and navigates to its details.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Searching for: {name}\")\n",
        "            search_input = self.selectors['search']['input']\n",
        "            \n",
        "            # Type with human-like delay\n",
        "            self.page.fill(search_input, \"\")\n",
        "            self.page.type(search_input, name, delay=random.randint(50, 150))\n",
        "            self.page.press(search_input, \"Enter\")\n",
        "            \n",
        "            # Wait for search results or direct redirect\n",
        "            self.page.wait_for_load_state(\"networkidle\")\n",
        "            \n",
        "            # Use a loop to wait for either results or detail page\n",
        "            for _ in range(5):\n",
        "                if self.page.locator(self.selectors['search']['recommendation_item']).count() > 0:\n",
        "                    logger.info(\"Multiple results found. Selecting the first one.\")\n",
        "                    self.page.click(self.selectors['search']['recommendation_item'] + \" \" + self.selectors['search']['recommendation_link'])\n",
        "                    self.page.wait_for_load_state(\"networkidle\")\n",
        "                    random_delay(2, 4)\n",
        "                    break\n",
        "                elif \"!1s\" in self.page.url or \"ChIJ\" in self.page.url:\n",
        "                    break\n",
        "                random_delay(1, 2)\n",
        "            \n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during search: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _extract_from_html(self, html_content, pattern, group=1, default=None):\n",
        "        \"\"\"Helper function to extract data from HTML using regex.\"\"\"\n",
        "        try:\n",
        "            match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(group)\n",
        "            return default\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error extracting with pattern: {e}\")\n",
        "            return default\n",
        "\n",
        "    def _get_place_id(self, html_content, metadata=None):\n",
        "        \"\"\"Extracts the Google Place ID.\"\"\"\n",
        "        if metadata and metadata.get('place_id'):\n",
        "            return metadata['place_id']\n",
        "\n",
        "        # Fall back to searching HTML for ChIJ pattern\n",
        "        place_id = self._extract_from_html(html_content, r'(ChIJ[a-zA-Z0-9_-]{20,})', 1)\n",
        "        return place_id\n",
        "\n",
        "    def get_place_details(self, name):\n",
        "        \"\"\"Extracts basic info about the place.\"\"\"\n",
        "        page_html = self.page.content()\n",
        "        place_id = self._get_place_id(page_html)\n",
        "        if not place_id:\n",
        "            place_id = extract_place_id_from_url(self.page.url)\n",
        "\n",
        "        lat, lng = extract_lat_long_from_url(self.page.url)\n",
        "\n",
        "        details = {\n",
        "            'place_id': place_id,\n",
        "            'place_url': self.page.url,\n",
        "            'nama_tempat': name,\n",
        "            'latitude': lat,\n",
        "            'longitude': lng,\n",
        "            'rating_total': None,\n",
        "            'ulasan_total': None,\n",
        "            'alamat': None,\n",
        "            'website': None,\n",
        "            'telepon': None,\n",
        "            'description': None,\n",
        "            'is_spending': False,\n",
        "            'competitors': None,\n",
        "            'can_claim': False,\n",
        "            'owner': None,\n",
        "            'featured_image': None,\n",
        "            'main_category': None,\n",
        "            'categories': None,\n",
        "            'workday_timing': None,\n",
        "            'is_temporarily_closed': False,\n",
        "            'is_permanently_closed': False,\n",
        "            'closed_on': None,\n",
        "            'review_keywords': None\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            sel = self.selectors['place_details']\n",
        "            \n",
        "            # Wait for any detail element to ensure page is loaded\n",
        "            try:\n",
        "                self.page.wait_for_selector(sel['name'], timeout=10000)\n",
        "            except:\n",
        "                logger.warning(\"Main name element not found within timeout.\")\n",
        "\n",
        "            # Name check (verify redirect)\n",
        "            if self.page.locator(sel['name']).count() > 0:\n",
        "                actual_name = self.page.text_content(sel['name'])\n",
        "                details['actual_name'] = actual_name.strip() if actual_name else name\n",
        "            else:\n",
        "                details['actual_name'] = name\n",
        "            \n",
        "            # Rating\n",
        "            rating_loc = self.page.locator(sel['rating'])\n",
        "            if rating_loc.count() > 0:\n",
        "                rating_text = rating_loc.first.text_content()\n",
        "                details['rating_total'] = rating_text.strip().replace(',', '.') if rating_text else None\n",
        "                \n",
        "            # Reviews count\n",
        "            reviews_count_loc = self.page.locator(sel['reviews_count'])\n",
        "            if reviews_count_loc.count() > 0:\n",
        "                # Try getting from aria-label first as it's cleaner\n",
        "                aria_label = reviews_count_loc.first.get_attribute(\"aria-label\")\n",
        "                if aria_label:\n",
        "                    count = \"\".join(filter(str.isdigit, aria_label))\n",
        "                    details['ulasan_total'] = int(count) if count else 0\n",
        "                else:\n",
        "                    reviews_count_text = reviews_count_loc.first.text_content()\n",
        "                    if reviews_count_text:\n",
        "                        count = \"\".join(filter(str.isdigit, reviews_count_text))\n",
        "                        details['ulasan_total'] = int(count) if count else 0\n",
        "                \n",
        "            # Optional info - Use both text_content and aria-label fallbacks\n",
        "            for key, selector in [('alamat', 'address'), ('website', 'website'), ('telepon', 'phone')]:\n",
        "                loc = self.page.locator(sel[selector])\n",
        "                if loc.count() > 0:\n",
        "                    # Prefer text inside if available, else aria-label\n",
        "                    txt = loc.first.text_content()\n",
        "                    if not txt or len(txt.strip()) < 5: # Some buttons only have icons/aria-label\n",
        "                        aria = loc.first.get_attribute(\"aria-label\")\n",
        "                        if aria:\n",
        "                            # Clean \"Alamat: \", \"Telepon: \", etc.\n",
        "                            details[key] = aria.split(\":\")[-1].strip()\n",
        "                        else:\n",
        "                            details[key] = txt.strip() if txt else None\n",
        "                    else:\n",
        "                        details[key] = txt.strip()\n",
        "            \n",
        "            # New Field Extractions\n",
        "            \n",
        "            # Rating & Review Count\n",
        "            rating_loc = self.page.locator(\"div.F7nice\")\n",
        "            if rating_loc.count() > 0:\n",
        "                 r_text = rating_loc.first.text_content().strip()\n",
        "                 # Expected format: \"4.5(2,530)\" or \"4.5(2.530)\"\n",
        "                 # rating_match Example text: 4.8(2,530)\n",
        "                 # Adjust regex to handle whitespace and ensure first group is rating, second group is count\n",
        "                 rating_match = re.search(r\"([\\d\\,]+[\\.\\,]?[\\d]*)\\s*\\(([\\d\\,\\.]+)\\)\", r_text)\n",
        "                 if rating_match:\n",
        "                     details['rating_total'] = rating_match.group(1).replace(',', '.') # Normalize rating to float format\n",
        "                     details['ulasan_total'] = rating_match.group(2).replace('.', '').replace(',', '') # Normalize count\n",
        "                 else:\n",
        "                     # Fallback if regex fails but rating exists\n",
        "                     details['rating_total'] = r_text\n",
        "            \n",
        "            # Description\n",
        "            desc_loc = self.page.locator(\"div.PYvSYb\")\n",
        "            if desc_loc.count() > 0:\n",
        "                details['description'] = desc_loc.first.text_content().strip()\n",
        "                \n",
        "            # Is Spending (Sponsored)\n",
        "            details['is_spending'] = self.page.locator(\"div:has-text('Disponsori')\").count() > 0\n",
        "            \n",
        "            # Can Claim\n",
        "            details['can_claim'] = self.page.locator(\"a[data-item-id='merchant']\").count() > 0\n",
        "            \n",
        "            # Categories\n",
        "            cat_loc = self.page.locator(\"button.DkEaL\")\n",
        "            if cat_loc.count() > 0:\n",
        "                cats = [c.strip() for c in cat_loc.all_text_contents() if c.strip()]\n",
        "                if cats:\n",
        "                    details['main_category'] = cats[0]\n",
        "                    details['categories'] = \", \".join(cats)\n",
        "                    \n",
        "            # Featured Image\n",
        "            img_loc = self.page.locator(\"button.aoRNLd img\")\n",
        "            if img_loc.count() > 0:\n",
        "                details['featured_image'] = img_loc.first.get_attribute(\"src\")\n",
        "                \n",
        "            # Closure status\n",
        "            details['is_temporarily_closed'] = self.page.locator(\"span:has-text('Tutup sementara')\").count() > 0\n",
        "            details['is_permanently_closed'] = self.page.locator(\"span:has-text('Tutup permanen')\").count() > 0\n",
        "            \n",
        "            # Workday Timing (attempts to extract the aria-label of the schedule dropdown)\n",
        "            timing_loc = self.page.locator(\"div[aria-label*='Sembunyikan jam buka'], div[aria-label*='Tampilkan jam buka']\")\n",
        "            if timing_loc.count() > 0:\n",
        "                 details['workday_timing'] = timing_loc.first.get_attribute(\"aria-label\")\n",
        "            \n",
        "            # --- XPath Fallbacks for Details ---\n",
        "            xf = self.selectors.get('xpath_fallbacks', {})\n",
        "            if not details.get('alamat') and xf.get('address'):\n",
        "                details['alamat'] = self.page.text_content(xf['address']).strip() if self.page.locator(xf['address']).count() > 0 else details['alamat']\n",
        "            if not details.get('website') and xf.get('website'):\n",
        "                details['website'] = self.page.text_content(xf['website']).strip() if self.page.locator(xf['website']).count() > 0 else details['website']\n",
        "            if not details.get('telepon') and xf.get('phone'):\n",
        "                details['telepon'] = self.page.text_content(xf['phone']).strip() if self.page.locator(xf['phone']).count() > 0 else details['telepon']\n",
        "            if not details.get('rating_total') and xf.get('rating'):\n",
        "                details['rating_total'] = self.page.text_content(xf['rating']).strip() if self.page.locator(xf['rating']).count() > 0 else details['rating_total']\n",
        "            if not details.get('ulasan_total') and xf.get('reviews_count'):\n",
        "                count_txt = self.page.text_content(xf['reviews_count'])\n",
        "                if count_txt:\n",
        "                    count = \"\".join(filter(str.isdigit, count_txt))\n",
        "                    details['ulasan_total'] = int(count) if count else details['ulasan_total']\n",
        "            \n",
        "            logger.info(f\"Extracted details for: {details['actual_name']}\")\n",
        "            return details\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Some details could not be extracted: {e}\")\n",
        "            return details\n",
        "\n",
        "    def scrape_reviews(self, place_id, name, place_url=\"\"):\n",
        "        \"\"\"Navigates to reviews tab and scrapes them with infinite scroll.\"\"\"\n",
        "        reviews = []\n",
        "        try:\n",
        "            sel = self.selectors['reviews']\n",
        "            \n",
        "            # Click reviews tab - Try both aria-label and text\n",
        "            tab_locators = [\n",
        "                self.page.locator(sel['tab_button']),\n",
        "                self.page.locator(\"button:has-text('Ulasan')\"),\n",
        "                self.page.locator(\"div[role='tab']:has-text('Ulasan')\")\n",
        "            ]\n",
        "            \n",
        "            tab_clicked = False\n",
        "            for loc in tab_locators:\n",
        "                if loc.count() > 0 and loc.first.is_visible():\n",
        "                    logger.info(f\"Clicking reviews tab using locator: {loc}\")\n",
        "                    loc.first.click()\n",
        "                    tab_clicked = True\n",
        "                    break\n",
        "            \n",
        "            if not tab_clicked:\n",
        "                logger.error(\"Reviews tab not found or not clickable.\")\n",
        "                return []\n",
        "            \n",
        "            # Use explicit delay instead of networkidle which hangs on Google Maps\n",
        "            random_delay(3, 5)\n",
        "                \n",
        "            # Sort by newest\n",
        "            sort_btn = self.page.locator(sel['sort_button'])\n",
        "            xf = self.selectors.get('xpath_fallbacks', {})\n",
        "            \n",
        "            if sort_btn.count() == 0 and xf.get('sort_button'):\n",
        "                sort_btn = self.page.locator(xf['sort_button'])\n",
        "                \n",
        "            if sort_btn.count() > 0 and sort_btn.first.is_visible():\n",
        "                logger.info(\"Opening sort menu.\")\n",
        "                sort_btn.first.click()\n",
        "                random_delay(2, 3)\n",
        "                \n",
        "                # Wait for menu items and try multiple strategies for 'Terbaru'\n",
        "                try:\n",
        "                    self.page.wait_for_selector(\"div[role='menuitemradio'], div[role='menuitem']\", timeout=5000)\n",
        "                except:\n",
        "                    pass\n",
        "                    \n",
        "                newest_opt = self.page.locator(sel['sort_newest'])\n",
        "                if newest_opt.count() == 0:\n",
        "                    newest_opt = self.page.locator(\"div[role='menuitemradio']:has-text('Terbaru'), div[role='menuitem']:has-text('Terbaru')\")\n",
        "                if newest_opt.count() == 0:\n",
        "                    newest_opt = self.page.locator(\"text=Terbaru\")\n",
        "                if newest_opt.count() == 0 and xf.get('sort_newest'):\n",
        "                    newest_opt = self.page.locator(xf['sort_newest'])\n",
        "                    \n",
        "                if newest_opt.count() > 0:\n",
        "                    logger.info(f\"Selecting 'Terbaru' sort option.\")\n",
        "                    newest_opt.first.click()\n",
        "                    # Wait for reviews to refresh using manual delay rather than networkidle\n",
        "                    random_delay(4, 6)\n",
        "                else:\n",
        "                    logger.warning(\"Sort option 'Terbaru' not found.\")\n",
        "            else:\n",
        "                logger.warning(\"Sort button not found.\")\n",
        "                \n",
        "            # Infinite scroll logic - Use a more robust way to find the scrollable container\n",
        "            # In GMap, the scrollable list is typically the div with tabindex=\"-1\" inside the main role.\n",
        "            container_locator = self.page.locator(\"div.m6QErb.DxyBCb.kA9KIf.dS8AEf[tabindex='-1']\").first\n",
        "            \n",
        "            if container_locator.count() == 0:\n",
        "                container_locator = self.page.locator(\"div[role='main']\").locator(\"..\").locator(\"div.m6QErb[tabindex='-1']\").first\n",
        "            if container_locator.count() == 0:\n",
        "                container_locator = self.page.locator(\"div[role='main']\").first\n",
        "                \n",
        "            if container_locator.count() == 0:\n",
        "                logger.error(f\"Review container not found.\")\n",
        "                return []\n",
        "\n",
        "            last_height = container_locator.evaluate(\"node => node.scrollHeight\")\n",
        "\n",
        "            max_scrolls = int(os.getenv(\"SCROLL_RETRY\", \"5\")) * 10\n",
        "            scroll_count = 0\n",
        "            \n",
        "            while scroll_count < max_scrolls:\n",
        "                # Scroll down using the correct element\n",
        "                container_locator.evaluate(\"node => node.scrollBy(0, 5000)\")\n",
        "                # Also try to scroll the last review item into view if possible\n",
        "                try:\n",
        "                    last_item = self.page.locator(sel['item']).last\n",
        "                    if last_item.count() > 0:\n",
        "                        last_item.scroll_into_view_if_needed(timeout=1000)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                \n",
        "                random_delay(1.5, 3)\n",
        "                \n",
        "                # Check for 'Ulasan lainnya' button (Pagination)\n",
        "                more_reviews_btn = self.page.locator(sel['more_reviews_button'])\n",
        "                if more_reviews_btn.count() > 0 and more_reviews_btn.first.is_visible():\n",
        "                    logger.info(\"Clicking 'Ulasan lainnya' button.\")\n",
        "                    more_reviews_btn.first.click()\n",
        "                    random_delay(2, 4)\n",
        "                \n",
        "                new_height = container_locator.evaluate(\"node => node.scrollHeight\")\n",
        "                if new_height == last_height:\n",
        "                    # Give it one more chance with a slightly bigger scroll\n",
        "                    container_locator.evaluate(\"node => node.scrollBy(0, 5000)\")\n",
        "                    random_delay(1, 2)\n",
        "                    new_height = container_locator.evaluate(\"node => node.scrollHeight\")\n",
        "                    if new_height == last_height:\n",
        "                        logger.info(\"Reached the end of the list or stuck.\")\n",
        "                        break\n",
        "                    \n",
        "                last_height = new_height\n",
        "                scroll_count += 1\n",
        "                \n",
        "                # Check if we have enough reviews based on date or count threshold\n",
        "                current_count = self.page.locator(sel['item']).count()\n",
        "                max_reviews = int(os.getenv(\"MAX_REVIEWS\", \"100\"))\n",
        "                if max_reviews > 0 and current_count >= max_reviews: \n",
        "                    break\n",
        "                \n",
        "            logger.info(f\"Finished scrolling/clicking. Found {self.page.locator(sel['item']).count()} potential reviews.\")\n",
        "            \n",
        "            # Extract data from review items\n",
        "            review_items = self.page.locator(sel['item']).all()\n",
        "            for item in review_items:\n",
        "                try:\n",
        "                    # Click 'More' if exists to expand long text\n",
        "                    more_btn = item.locator(sel['more_button'])\n",
        "                    if more_btn.count() > 0:\n",
        "                        more_btn.first.click()\n",
        "                        random_delay(0.5, 1)\n",
        "                        \n",
        "                    review_data = {\n",
        "                        'place_id': place_id,\n",
        "                        'place_url': place_url,\n",
        "                        'nama_tempat': name,\n",
        "                        'review_id': item.get_attribute(\"data-review-id\"),\n",
        "                        'author_name': item.locator(sel['author']).text_content(),\n",
        "                        'rating_ulasan': item.locator(sel['rating']).get_attribute(\"aria-label\"),\n",
        "                        'tanggal_raw': item.locator(sel['date']).text_content(),\n",
        "                        'isi_review': item.locator(sel['text']).text_content() if item.locator(sel['text']).count() > 0 else \"\",\n",
        "                        # Owner reply detection\n",
        "                        'balasan_pemilik': \"\",\n",
        "                        'tanggal_balasan_raw': \"\"\n",
        "                    }\n",
        "                    \n",
        "                    # Small logic for owner reply (usually nested or separate div with same text style but different container)\n",
        "                    # This depends on local language. In REVIEWS_PAGE_HTML we saw 'Balasan dari pemilik' pattern.\n",
        "                    # For now, we use a simple approach or fallback.\n",
        "                    \n",
        "                    reviews.append(review_data)\n",
        "                except Exception as ex:\n",
        "                    logger.warning(f\"Error extracting single review: {ex}\")\n",
        "                    \n",
        "            return reviews\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during review scraping: {e}\")\n",
        "            return reviews\n",
        "\n",
        "\n",
        "logger = setup_logger()\n",
        "\n",
        "def run_scraper(places_list, max_reviews=100):\n",
        "    os.environ[\"MAX_REVIEWS\"] = str(max_reviews)\n",
        "    \n",
        "    logger.info(f\"Starting scrape for {len(places_list)} places.\")\n",
        "    browser_mgr = BrowserManager(headless=True)\n",
        "    processor = DataProcessor()\n",
        "    \n",
        "    errors = []\n",
        "    \n",
        "    try:\n",
        "        page = browser_mgr.start_browser()\n",
        "        scraper = GoogleMapsScraper(page)\n",
        "        \n",
        "        for place_name in places_list:\n",
        "            try:\n",
        "                logger.info(f\"--- Processing: {place_name} ---\")\n",
        "                page.goto(\"https://www.google.com/maps\")\n",
        "                page.wait_for_load_state(\"networkidle\")\n",
        "                \n",
        "                if scraper.search_place(place_name):\n",
        "                    random_delay(2, 4)\n",
        "                    details = scraper.get_place_details(place_name)\n",
        "                    place_id = details.get('place_id')\n",
        "                    place_url = details.get('place_url', page.url)\n",
        "                    \n",
        "                    raw_reviews = scraper.scrape_reviews(place_id, place_name, place_url)\n",
        "                    processed_reviews = processor.process_reviews(raw_reviews, details)\n",
        "                    \n",
        "                    processor.export_to_csv(processed_reviews)\n",
        "                    logger.info(f\"Successfully scraped and exported {len(processed_reviews)} reviews for {place_name}.\")\n",
        "                    \n",
        "                    if not place_id:\n",
        "                        logger.warning(f\"Note: Place ID was not found for {place_name}, but continuing with name/link.\")\n",
        "                else:\n",
        "                    logger.warning(f\"Place not found: {place_name}\")\n",
        "                    errors.append({\"place_name\": place_name, \"error\": \"Search failed\"})\n",
        "                \n",
        "                random_delay(5, 10)\n",
        "                \n",
        "                if places_list.index(place_name) % 2 == 1:\n",
        "                    logger.info(\"Switching to a fresh browser context.\")\n",
        "                    page = browser_mgr.get_new_context()\n",
        "                    scraper.page = page\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error processing {place_name}: {e}\")\n",
        "                errors.append({\"place_name\": place_name, \"error\": str(e)})\n",
        "\n",
        "        processor.export_errors(errors)\n",
        "\n",
        "    finally:\n",
        "        browser_mgr.close_browser()\n",
        "        logger.info(\"Scraping process completed. Silakan periksa file CSV di folder output_data/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Jalankan Scraper\n",
        "Masukkan nama tempat yang ingin discrape ke dalam list di bawah ini. Atur `max_reviews` sesuai kebutuhan Anda.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "\n",
        "tempat_yang_ingin_di_scrape = [\n",
        "    \"SiCepat Ekspres Indonesia Pusat\",\n",
        "    \"SiCepat Ekspres General Affair Office\",\n",
        "    \"SiCepat Ekspres Menteng\",\n",
        "    \"SiCepat Ekspres Kemayoran\",\n",
        "    \"SiCepat Ekspres Kebayoran Lama\"\n",
        "]\n",
        "\n",
        "# Jalankan scraper dengan limit 100 review per tempat (gunakan 0 untuk max tanpa henti)\n",
        "run_scraper(tempat_yang_ingin_di_scrape, max_reviews=100)\n",
        "\n",
        "# Untuk mendownload/menampilkan file CSV terakhir\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "list_of_files = glob.glob('output_data/*.csv')\n",
        "if list_of_files:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Data tersimpan di {latest_file}. Anda dapat mendownloadnya dari tab Files di sisi kiri layar.\")\n",
        "    df = pd.read_csv(latest_file)\n",
        "    display(df.head())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Google_Maps_Scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}